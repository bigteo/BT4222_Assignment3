{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3_A0203989N.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sEknxYKtjfzK",
        "F7-7B5iwrNYB",
        "2Uf123B7jiCD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QelXyP-0hiT5"
      },
      "source": [
        "# BT4222 Assignment 3\n",
        "By Teo Zhi Feng (A0203989N)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMzTMwKfhvdl"
      },
      "source": [
        "# 1 Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h24U7PflBFyV"
      },
      "source": [
        "#### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfXQC4KvABlY"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-UQHsr9hhtH"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from numpy.random import seed\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Conv1D, Dense, Dropout, Embedding, Flatten, GlobalMaxPooling1D,  LSTM, MaxPooling1D, TextVectorization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "import string\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2v8-kRR-1cq"
      },
      "source": [
        "seed(1)\n",
        "set_seed(2)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdKjHmN974Sj"
      },
      "source": [
        "#### Determine parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Oa_WfbyxF2Y"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 128 # Select a batch size to fully utilise GPU memory, or 16 or 32\n",
        "\n",
        "embedding_dims = 200 #50 # emddeding for each word\n",
        "maxlen = 800 # max number of words in the review\n",
        "\n",
        "filters = 250\n",
        "kernel_size = 3 # size of the 1D conv. layer\n",
        "hidden_dims = 128 # number of dimensions\n",
        "\n",
        "metrics = ['accuracy', \"Precision\", \"Recall\"]\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ByVIc8d0aC0"
      },
      "source": [
        "#### Import IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKH4gT460Pv9",
        "outputId": "93816a14-1f2c-489f-e839-047b503b1c7a"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  59.7M      0  0:00:01  0:00:01 --:--:-- 59.7M\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmRrVV4osYQi"
      },
      "source": [
        "#### Inspect folder directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGUHXRDqmWHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de7c4212-fec6-4f72-cbea-f903a7b4c727"
      },
      "source": [
        "!ls aclImdb\n",
        "!ls aclImdb/test\n",
        "!ls aclImdb/train"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n",
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n",
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMHf2EeonDTg"
      },
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te5c4IfYsj1N"
      },
      "source": [
        "#### Import all datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqnfXcXJnDPo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "898a6505-60b4-4d82-f659-68e233fe89f4"
      },
      "source": [
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/test\",\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 files belonging to 2 classes.\n",
            "Using 0 files for training.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-38f5df622ec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1337\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/text_dataset.py\u001b[0m in \u001b[0;36mtext_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, batch_size, max_length, shuffle, seed, validation_split, subset, follow_links)\u001b[0m\n\u001b[1;32m    155\u001b[0m       file_paths, labels, validation_split, subset)\n\u001b[1;32m    156\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No text files found.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m   dataset = paths_and_labels_to_dataset(\n",
            "\u001b[0;31mValueError\u001b[0m: No text files found."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8RbfM0ysoGD"
      },
      "source": [
        "#### Inspect dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTE61hxtsCg-"
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLygH6phvW2g"
      },
      "source": [
        "#### Define list to store model performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG68CbqbvWKg"
      },
      "source": [
        "results = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u1MUVaKoMq9"
      },
      "source": [
        "# 2 Data Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsA9pKy4tLy-"
      },
      "source": [
        "#### Vectorise data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPim7cIQnDGI"
      },
      "source": [
        "# Define custom function\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )\n",
        "\n",
        "# Model constants.\n",
        "max_features = 20000\n",
        "sequence_length = 500\n",
        "\n",
        "# Instantiate text vectorisation layer to normalize, split, and map strings to integers.\n",
        "# We also set an explicit maximum sequence length, since the CNNs later in our model won't support ragged sequences.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# With vocab layer, call `adapt` on a text-only dataset to create the vocabulary. \n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG2l-87M3sRL"
      },
      "source": [
        "print(len(vectorize_layer.get_vocabulary()))\n",
        "print(vectorize_layer.get_vocabulary()[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB1DJBInpWnO"
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pxAX7eO_lbq"
      },
      "source": [
        "#### Inspect vectorised dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3CnRHv3pj_N"
      },
      "source": [
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review\", first_review)\n",
        "print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6pWc6HXkcvA"
      },
      "source": [
        "# 3 Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW9Cp32i3FYV"
      },
      "source": [
        "#### Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FCmN0HmyPhw"
      },
      "source": [
        "model_cnn = Sequential()\n",
        "\n",
        "model_cnn.add(Embedding(max_features,\n",
        "                     embedding_dims,\n",
        "                     input_length=maxlen))\n",
        "model_cnn.add(Dropout(0.5))\n",
        "\n",
        "# Convolution and pooling layer 1\n",
        "model_cnn.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='same',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model_cnn.add(GlobalMaxPooling1D())\n",
        "\n",
        "##### OTHER CONVOLUTION LAYERS #####\n",
        "# model_cnn.add(Conv1D(128,\n",
        "#                  7,\n",
        "#                  padding='same',\n",
        "#                  activation='relu',\n",
        "#                  strides=3,\n",
        "#                  name='convolution1'))\n",
        "\n",
        "\n",
        "# Flatten before connecting to dense layer \n",
        "model_cnn.add(Flatten())\n",
        "\n",
        "model_cnn.add(Dense(hidden_dims, activation='relu'))\n",
        "model_cnn.add(Dropout(0.5))\n",
        "\n",
        "# Project onto a single unit, dense output layer and apply sigmoid activation function\n",
        "# to make 0 or 1 predictions for the two classes (positive or negative).\n",
        "model_cnn.add(Dense(1, activation='sigmoid', name='predictions'))\n",
        "\n",
        "# Use binary_crossentropy loss function as it is a binary classification problem \n",
        "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V8hHfueRZhz"
      },
      "source": [
        "#### Visualise model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwuGTzOe2kuN"
      },
      "source": [
        "model_cnn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO4llMMDRUha"
      },
      "source": [
        "plot_model(model_cnn, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnWGkwij3IrO"
      },
      "source": [
        "#### Train model using train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnJnZDLJkdtj"
      },
      "source": [
        "# Fit model using train dataset\n",
        "history_cnn = model_cnn.fit(train_ds,\n",
        "              epochs=epochs,\n",
        "              verbose = 1,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=val_ds,\n",
        "              callbacks=[EarlyStopping(monitor='val_loss',patience=3, min_delta=0.0001)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqQpw_VufOXy"
      },
      "source": [
        "figure, axis = plt.subplots(2, sharex=True)\n",
        "\n",
        "axis[0].plot(history_cnn.history['loss'], label='train')\n",
        "axis[0].plot(history_cnn.history['val_loss'], label='validation')\n",
        "axis[0].legend()\n",
        "axis[0].set_title('Loss')\n",
        "\n",
        "axis[1].plot(history_cnn.history['accuracy'], label='train')\n",
        "axis[1].plot(history_cnn.history['val_accuracy'], label='validation')\n",
        "axis[1].legend()\n",
        "axis[1].set_title('Accuracy')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enb8etY3v81x"
      },
      "source": [
        "#### Evaluate model performance on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Q6BCam2Wi2"
      },
      "source": [
        "metrics_cnn = model_cnn.evaluate(test_ds,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJWo8LJAhaWK"
      },
      "source": [
        "loss_cnn = round(metrics_cnn[0], 3)\n",
        "accuracy_cnn = round(metrics_cnn[1], 3)\n",
        "precision_cnn = round(metrics_cnn[2], 3)\n",
        "recall_cnn = round(metrics_cnn[3], 3)\n",
        "\n",
        "print('Model performance on test set:\\nLoss: {}\\nAccuracy: {}\\nPrecision: {}\\nRecall: {}\\n'.format(loss_cnn, accuracy_cnn, precision_cnn, recall_cnn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOFLReASwCHP"
      },
      "source": [
        "results['CNN'] = {\n",
        "    'Loss': loss_cnn,\n",
        "    'Accuracy': accuracy_cnn,\n",
        "    'Precision': precision_cnn, \n",
        "    'Recall': recall_cnn,\n",
        "    'F1-score': round( 2*((precision_cnn * recall_cnn) / (precision_cnn + recall_cnn)), 3 )\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0iXLoWpkd6-"
      },
      "source": [
        "# 4 Long Short-Term Memory (LSTM) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctvh_HOX0foK"
      },
      "source": [
        "### Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPPlmifnkgAZ"
      },
      "source": [
        "model_lstm = Sequential()\n",
        "\n",
        "model_lstm.add(Embedding(max_features,\n",
        "                     embedding_dims,\n",
        "                     input_length=maxlen))\n",
        "\n",
        "model_lstm.add(LSTM(120, return_sequences=True))\n",
        "model_lstm.add(GlobalMaxPooling1D())\n",
        "\n",
        "model_lstm.add(Dense(1, activation='sigmoid', name='predictions'))\n",
        "\n",
        "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFNhKJDrRKTp"
      },
      "source": [
        "#### Visualise model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0breL5Izr4S"
      },
      "source": [
        "model_lstm.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAcYTf8xQil7"
      },
      "source": [
        "plot_model(model_lstm, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De53SxX-z8JF"
      },
      "source": [
        "#### Train model using train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_H0Y9exz8JF"
      },
      "source": [
        "# Fit model using train dataset\n",
        "history_lstm = model_lstm.fit(train_ds,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=val_ds,\n",
        "              callbacks=[EarlyStopping(monitor='val_loss',patience=3, min_delta=0.0001)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvLEeIXEz8JG"
      },
      "source": [
        "figure, axis = plt.subplots(2, sharex=True)\n",
        "\n",
        "axis[0].plot(history_lstm.history['loss'], label='train')\n",
        "axis[0].plot(history_lstm.history['val_loss'], label='validation')\n",
        "axis[0].legend()\n",
        "axis[0].set_title('Loss')\n",
        "\n",
        "axis[1].plot(history_lstm.history['accuracy'], label='train')\n",
        "axis[1].plot(history_lstm.history['val_accuracy'], label='validation')\n",
        "axis[1].legend()\n",
        "axis[1].set_title('Accuracy')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pRbr9Snz8JG"
      },
      "source": [
        "#### Evaluate model performance on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb1_h4ekz8JG"
      },
      "source": [
        "metrics_lstm = model_lstm.evaluate(test_ds,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMytDZS9z8JG"
      },
      "source": [
        "loss_lstm = round(metrics_lstm[0], 3)\n",
        "accuracy_lstm = round(metrics_lstm[1], 3)\n",
        "precision_lstm = round(metrics_lstm[2], 3)\n",
        "recall_lstm = round(metrics_lstm[3], 3)\n",
        "\n",
        "print('Model performance on test set:\\nLoss: {}\\nAccuracy: {}\\nPrecision: {}\\nRecall: {}\\n'.format(loss_lstm, accuracy_lstm, precision_lstm, recall_lstm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLzo0Kgfz8JG"
      },
      "source": [
        "results['LSTM'] = {\n",
        "    'Loss': loss_lstm,\n",
        "    'Accuracy': accuracy_lstm,\n",
        "    'Precision': precision_lstm, \n",
        "    'Recall': recall_lstm,\n",
        "    'F1-score': round( 2*((precision_lstm * recall_lstm) / (precision_lstm + recall_lstm)), 3 )\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmoJdoAH9FGd"
      },
      "source": [
        "#### Get predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlPMHEIy9Iiy"
      },
      "source": [
        "threshold = 0.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0BJDAU79Ke6"
      },
      "source": [
        "predictions = np.array(model_lstm.predict(test_ds) > threshold, dtype=np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_zvmUM19SRC"
      },
      "source": [
        "test_labels = np.array([k[1].numpy() for k in test_ds.unbatch()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "funP2PxJ9ZP6"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "print('Precision: {}'.format(precision_score(test_labels, predictions)))\n",
        "print('Recall: {}'.format(recall_score(test_labels, predictions)))\n",
        "print('F1: {}'.format(f1_score(test_labels, predictions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO-uFahvkgG8"
      },
      "source": [
        "# 5 Model Comparison\n",
        "Comparing CNN and LSTM, the results are summarised in the DataFrame below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKQ0ebz2kiIg"
      },
      "source": [
        "results = pd.DataFrame(results)\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Z_3w0MM7-Y"
      },
      "source": [
        "From the results, we see that LSTM performed better in terms of precision, recall and F1 score because XXX."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7iDzOQoAf7m"
      },
      "source": [
        "CNN is useful to recognise patterns across space and it is good at extracting local and position-invariant features. \n",
        "\n",
        "Furthermore, CNN is very fast.\n",
        "\n",
        "Hence, CNN is better when feature detection in text is more important. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjFYxz_ZAf3O"
      },
      "source": [
        "On the other hand, RNN is useful to recognise patterns across time and it capture long-term dependencies between word sequences.\n",
        "\n",
        "RNN is useful in analysing sequential data when the current step is related with the previous steps. RNN is ideal for datasets with a time component and natural language processing. In particular, RNN performs well when sequential informaion is clearly important, which reduces the likelihood that the meaning being misinterpreted or the grammer being incorrect.\n",
        "\n",
        "Hence, RNN is better whe sequential modeling is more important because it \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g5O4FdA_DeX"
      },
      "source": [
        "# 6 Insights\n",
        "The last 2 points will be given if you have provided any deeper insights. (For example but not limited to, provide empirical evidence for why one method is better than the other one; how the change of the network structure might influence the performance; how the data characteristics might influence the performance; any other aspects you want to highlight)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H22pRXrMaic"
      },
      "source": [
        "#### Insight 1: Wide vs deep neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV0LS1vjNVFP"
      },
      "source": [
        "Deep neural network (6 layers of CNN followed by 3 fully connected layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYZmiZhnMfrX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ia2gQq-NcYw"
      },
      "source": [
        "Very deep neural network (26 layers of CNN followed by 3 fully connected layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VY608QYNc3x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5538vzD8ySm"
      },
      "source": [
        "#### Insight 2: Different CNN/LSTM models\n",
        "Different scholars have tried different combinations of CNN and LSTM layers in order to improve model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrm2sTTcFXYZ"
      },
      "source": [
        "Variation 1: CNN + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9RWcKjwN0n7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhf-_Bf0N1Bs"
      },
      "source": [
        "Variation 2: Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cla3MvlTN-Ew"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c18TXe3mN9Kn"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoInZM8WN85b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzH_BBGc8zVS"
      },
      "source": [
        "#### Insight 2: Adjust threshold value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlnDXeeU8zIw"
      },
      "source": [
        "threshold = [x/10 for x in range(1,10)]\n",
        "test_labels = np.array([k[1].numpy() for k in test_ds.unbatch()])\n",
        "results_threshold_cnn = {'Accuracy': {}, 'Precision': {}, 'Recall': {}, 'F1': {}}\n",
        "results_threshold_lstm = {'Accuracy': {}, 'Precision': {}, 'Recall': {}, 'F1': {}}\n",
        "\n",
        "for t in threshold:\n",
        "  predictions_cnn = np.array(model_cnn.predict(test_ds) > t, dtype=np.int32)\n",
        "  results_threshold_cnn['Accuracy']['CNN_' + str(t)] = accuracy_score(test_labels, predictions_cnn)\n",
        "  results_threshold_cnn['Precision']['CNN_' + str(t)] = precision_score(test_labels, predictions_cnn)                           \n",
        "  results_threshold_cnn['Recall']['CNN_' + str(t)] = recall_score(test_labels, predictions_cnn)\n",
        "  results_threshold_cnn['F1']['CNN_' + str(t)] = f1_score(test_labels, predictions_cnn)\n",
        "\n",
        "  predictions_lstm = np.array(model_cnn.predict(test_ds) > t, dtype=np.int32)\n",
        "  results_threshold_lstm['Accuracy']['LSTM_' + str(t)] = accuracy_score(test_labels, predictions_lstm)\n",
        "  results_threshold_lstm['Precision']['LSTM_' + str(t)] = precision_score(test_labels, predictions_lstm)\n",
        "  results_threshold_lstm['Recall']['LSTM_' + str(t)] = recall_score(test_labels, predictions_lstm)\n",
        "  results_threshold_lstm['F1']['LSTM_' + str(t)] = f1_score(test_labels, predictions_lstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5ks7Tss2mLB"
      },
      "source": [
        "results_threshold_cnn = pd.DataFrame(results_threshold_cnn)\n",
        "results_threshold_cnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgzSW4Ns8akt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiZ8RdQhFbbi"
      },
      "source": [
        "Variation 2: CNN + LSTM + CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38RIRbbTFd0a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilzgkCgqD-c6"
      },
      "source": [
        "#### Insight 3: Word embedding techniques\n",
        "Different word embedding techniques may affect the model performances. Other types of word embedding techniques could have been used, such as word2vec."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7KLf35u8aiA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u23p2T1s_aTQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geT-G1sU_aIt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZP_YIfV_aCP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFKQdg278aaO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYiUSmMUauln"
      },
      "source": [
        "https://medium.com/@mrunal68/text-sentiments-classification-with-cnn-and-lstm-f92652bc29fd\n",
        "\n",
        "- CNN does not depend on the computations of the previous time steps and therefore allow parallelization over every element in a sequence.\n",
        "- CNN obtains essential features of text through pooling but it is difficult to obtain contextual information.\n",
        "\n",
        "- LSTM, and RNN in general, maintains a hidden state of the entire past that prevents parallel computation within a sequence.\n",
        "- However, it can potentially lead to bias "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL8uFXmjdCDm"
      },
      "source": [
        "https://towardsdatascience.com/text-classification-rnns-or-cnn-s-98c86a0dd361"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zEWZm3yMBEv"
      },
      "source": [
        "results_threshold_lstm = pd.DataFrame(results_threshold_lstm)\n",
        "results_threshold_lstm"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}